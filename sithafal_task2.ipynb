{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTUcVbOkDncsKNWzAJNZW7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pokasanthi/Sithafal-tasks/blob/main/sithafal_task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrNna32ns2as",
        "outputId": "993c02f8-7aa9-4b13-fb68-280693c4c478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 sentence-transformers faiss-cpu transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. Function to fetch and parse content from websites\n",
        "def fetch_website_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            text = \" \".join([p.get_text() for p in soup.find_all(\"p\")])  # Extract paragraphs\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"Failed to fetch {url}: {response.status_code}\")\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# List of example websites\n",
        "websites = [\n",
        "    \"https://www.uchicago.edu/\",\n",
        "    \"https://www.washington.edu/\",\n",
        "    \"https://www.stanford.edu/\",\n",
        "    \"https://und.edu/\"\n",
        "]\n",
        "\n",
        "# Fetch content from all websites\n",
        "website_texts = []\n",
        "for url in websites:\n",
        "    print(f\"Fetching content from: {url}\")\n",
        "    text = fetch_website_text(url)\n",
        "    if text:\n",
        "        website_texts.append((url, text[:1000]))  # Limit text length for simplicity\n",
        "        print(f\"Fetched {len(text)} characters from {url}\\n\")\n",
        "\n",
        "# 2. Chunk the text for embeddings\n",
        "def chunk_text(data, max_length=512):\n",
        "    chunks = []\n",
        "    for url, text in data:\n",
        "        for i in range(0, len(text), max_length):\n",
        "            chunk = text[i:i+max_length]\n",
        "            if len(chunk) > 50:  # Only include meaningful chunks\n",
        "                chunks.append((url, chunk))\n",
        "    return chunks\n",
        "\n",
        "# Chunk the website content\n",
        "chunked_data = chunk_text(website_texts)\n",
        "print(f\"Total chunks created: {len(chunked_data)}\")\n",
        "\n",
        "# 3. Generate embeddings for each chunk\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def generate_embeddings(chunked_data):\n",
        "    embeddings = []\n",
        "    for url, chunk in chunked_data:\n",
        "        emb = model.encode([chunk])[0]\n",
        "        embeddings.append((url, emb))\n",
        "    return embeddings\n",
        "\n",
        "embeddings_with_sources = generate_embeddings(chunked_data)\n",
        "\n",
        "# Prepare embeddings for FAISS\n",
        "embeddings = np.array([emb for _, emb in embeddings_with_sources]).astype('float32')\n",
        "sources = [url for url, _ in embeddings_with_sources]\n",
        "\n",
        "# 4. Store embeddings in FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n",
        "index.add(embeddings)\n",
        "print(f\"FAISS index created with {index.ntotal} embeddings.\")\n",
        "\n",
        "# Save sources (to map indices back to websites)\n",
        "np.save(\"sources.npy\", sources)\n",
        "\n",
        "# 5. Query and search the FAISS index\n",
        "def query_and_search(query, k=3):\n",
        "    query_emb = model.encode([query]).astype('float32')\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "    return distances, indices\n",
        "\n",
        "# 6. Summarize the top-k results\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def summarize_results(query, k=3):\n",
        "    distances, indices = query_and_search(query, k)\n",
        "    sources = np.load(\"sources.npy\", allow_pickle=True)\n",
        "\n",
        "    # Retrieve and print top-k results\n",
        "    top_chunks = []\n",
        "    for i in indices[0]:\n",
        "        if i < len(chunked_data):\n",
        "            url, chunk = chunked_data[i]\n",
        "            top_chunks.append(chunk)\n",
        "            print(f\"Source: {url}\\nContent: {chunk[:200]}...\\n\")\n",
        "\n",
        "    # Summarize combined top-k chunks\n",
        "    combined_text = \" \".join(top_chunks)\n",
        "    summary = summarizer(combined_text, max_length=130, min_length=30, do_sample=False)\n",
        "    print(\"\\n**Summary of Retrieved Content:**\")\n",
        "    print(summary[0]['summary_text'])\n",
        "\n",
        "# 7. Example Query\n",
        "query = \"What can I learn about universities?\"\n",
        "print(f\"Query: {query}\")\n",
        "summarize_results(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIlOJQnFuU45",
        "outputId": "50d40fe3-ddf9-4ea4-8e44-04cded6ddc50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching content from: https://www.uchicago.edu/\n",
            "Failed to fetch https://www.uchicago.edu/: 403\n",
            "Fetching content from: https://www.washington.edu/\n",
            "Fetched 1416 characters from https://www.washington.edu/\n",
            "\n",
            "Fetching content from: https://www.stanford.edu/\n",
            "Fetched 3519 characters from https://www.stanford.edu/\n",
            "\n",
            "Fetching content from: https://und.edu/\n",
            "Fetched 2753 characters from https://und.edu/\n",
            "\n",
            "Total chunks created: 6\n",
            "FAISS index created with 6 embeddings.\n",
            "Query: What can I learn about universities?\n",
            "Source: https://und.edu/\n",
            "Content: The University of North Dakota is the state's oldest and largest university. We offer\n",
            "                           225+ highly accredited on-campus and online degrees. Explore the causes and impact of c...\n",
            "\n",
            "Source: https://www.stanford.edu/\n",
            "Content:  Farm Science & Engineering Health & Medicine Science & Engineering Awards Science & Engineering Science & Engineering Preparing students to make meaningful contributions to society as engaged citizen...\n",
            "\n",
            "Source: https://www.stanford.edu/\n",
            "Content: \n",
            "      Other ways to search:\n",
            "        Map\n",
            "Profiles\n",
            " Stanford Explore Stanford Stanford was founded almost 150 years ago on a bedrock of societal purpose. Our mission is to contribute to the world by ed...\n",
            "\n",
            "\n",
            "**Summary of Retrieved Content:**\n",
            "The University of North Dakota is the state's oldest and largest university. We offer 225+ highly accredited on-campus and online degrees. Explore the causes and impact of criminal behavior and prepare to play a key role in criminal justice.\n"
          ]
        }
      ]
    }
  ]
}